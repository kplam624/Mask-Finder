{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bound-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from numpy import asarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "polished-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find and read image\n",
    "images_dir = cv2.imread(\"/Users/atemkuh/Documents/GitHub/Mask-Finder/dataset/with_mask/0_0_≈˙◊¢ 2020-02-23 132115.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sudden-lucas",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) /private/var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/pip-req-build-40fplvaz/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cbd1e6ce043a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#view image and convert into RGB using cv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.1) /private/var/folders/nz/vv4_9tw56nv9k3tkvyszvwg80000gn/T/pip-req-build-40fplvaz/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "#view image and convert into RGB using cv2\n",
    "plt.imshow(cv2.cvtColor(images_dir, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "stopped-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "#images_dir.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-artist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through dataset\n",
    "Images_directory = \"/Users/atemkuh/Documents/GitHub/Mask-Finder/dataset\"\n",
    "Classes = [\"with_mask\",\"without_mask\"]\n",
    "for category in Classes:\n",
    "    path = os.path.join(Images_directory, category)\n",
    "    for img in os.listdir(path):\n",
    "        images_dir = cv2.imread(os.path.join(path,img))\n",
    "        \n",
    "        plt.imshow(cv2.cvtColor(images_dir, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        # stop processing after first image\n",
    "        break\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-shanghai",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize images using imageNet \n",
    "img_size = 224\n",
    "new_img_array=cv2.resize(images_dir,(img_size,img_size))\n",
    "plt.imshow(cv2.cvtColor(images_dir, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert images to arrays \n",
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    for category in Classes:\n",
    "        path = os.path.join(Images_directory,category)\n",
    "        class_num = Classes.index(category)\n",
    "        \n",
    "        print(\"category\")\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "               # print(img, \", \", path)\n",
    "                images_dir    = cv2.imread(os.path.join(path,img))\n",
    "                \n",
    "                new_img_array = cv2.resize(images_dir,(img_size,img_size))\n",
    "                \n",
    "                training_data.append([new_img_array, class_num])\n",
    "              #  print(len(training_data))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "X =[]\n",
    "y=[]\n",
    "\n",
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X).reshape(-1,img_size,img_size,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle x\n",
    "pickle_out = open(\"X.pickle\",\"wb\")\n",
    "pickle.dump(X,pickle_out)\n",
    "pickle_out.close()\n",
    "#pickle y\n",
    "pickle_out = open(\"y.pickle\",\"wb\")\n",
    "pickle.dump(y,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ruled-major",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3d46d7144e80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#create and train deep learning model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#pre trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMobileNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Total params: 4,253,864\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#create and train deep learning model\n",
    "#pre trained model\n",
    "model = tf.keras.applications.MobileNet()\n",
    "model.summary() #Total params: 4,253,864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use existing solution by implementing transfer learning \"tuning\", \"weights\"\n",
    "base_input = model.layers[0].input\n",
    "base_output = model.layers[-4].output\n",
    "Flatten_layer =  layers.Flatten()(base_output)\n",
    "#classifier  can either be 0 or 1\n",
    "final_output = layers.Dense(1)(Flatten_layer)\n",
    "final_output = layers.Activation('sigmoid')(final_output)\n",
    "#new model\n",
    "\n",
    "new_model=keras.Model(inputs = base_input, outputs = final_output)\n",
    "new_model.summary()## display the sum of the new model #Total params: 3,229,889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup up configuration os classes('with_mask','without_mask')\n",
    "\n",
    "#compile new model\n",
    "new_model.compile(loss=\"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fit (X,Y, epochs = 1, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "frame = cv2.imread('demo.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_img =cv2.resize(frame,(224,224))\n",
    "final_img =np.expand_dims(final_img, axis=0)\n",
    "final_img = final_img/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions = new_model.predict(final_img)\n",
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Haar feature detector\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades+'haarcascade_frontalface_default.xml')\n",
    "#convert image from BGR to gray\n",
    "gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to detect faces\n",
    "\n",
    "face = faceCascade.detectMultiScale(gray,1.1,4)\n",
    "for x,y,w,h in face:\n",
    "    roi_gray = gray[y:y+h,x:x+w]\n",
    "    roi_color = frame[y:y+h,x:x+w]\n",
    "    cv2.rectangle(frame,(x,y),(x+w, y+h), (0,255,0),2)\n",
    "    faces = faceCascade.detectMultiScale(roi_gray)\n",
    "    if len(faces)==0:\n",
    "        print(\"face not detected\")\n",
    "    else:\n",
    "        for (ex,ey,ew,eh) in faces:\n",
    "            face_roi = roi_color[ey: ey+eh,ex:ex+ew]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop face\n",
    "plt.imshow(cv2.cvtColor(face_roi,cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_img =cv2.resize(frame,(224,224))\n",
    "final_img =np.expand_dims(final_img, axis=0)\n",
    "final_img = final_img/255.0\n",
    "Predictions = new_model.predict(final_img)\n",
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"haarcascade_frontalface_default.xml\"\n",
    "font_scale=1.5\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "rectangle_bgr = (255,255,255)\n",
    "img = np.zeros((500,500))\n",
    "text=\"some. text in a box!\"#text in a box\n",
    "#width and height\n",
    "(text_width, text_height) = cv2.getTextSize(text,font,fontScale=font_scale, thickness=1)[0]\n",
    "#set text position\n",
    "\n",
    "text_offset_x = 10\n",
    "text_offset_y = img.shape[0]-25\n",
    "\n",
    "#box coords\n",
    "\n",
    "box_coords =((text_offset_x, text_offset_y),(text_offset_x+text_width+2,text_offset_y - text_height - 2))\n",
    "cv2.rectangle (img,box_coords[0], box_coords[1],rectangle_bgr, cv2.FILLED)\n",
    "cv2.putText(img,text,(text_offset_x, text_offset_y), font, fontScale=font_scale, color = (0,0,0), thickness = 1)\n",
    "\n",
    "#setup video capture\n",
    "cap = cv2.VideoCapture(1)\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(0)#number of camera 0 for one camera present\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Please check camera or webcam\")\n",
    "    \n",
    "while True:\n",
    "    \n",
    "    ret,frame = cap.read()\n",
    "    \n",
    "    faceCascade =cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    face = faceCascade.detectMultiScale(gray,1.1,4)\n",
    "    for x,y,w,h in face:\n",
    "        roi_gray=gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), (255,0,0),2)\n",
    "        faces = faceCascade.detectMultiScale(roi_gray)\n",
    "        if len(faces)==0:\n",
    "            print (\"No face detected\")\n",
    "        else:\n",
    "            # crop our region of interest (ROI)\n",
    "            for (ex,ey,ew,eh) in faces:\n",
    "                face_roi in roi_color[ey: ey+eh, ex:ex+ew]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonAdv] *",
   "language": "python",
   "name": "conda-env-PythonAdv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
